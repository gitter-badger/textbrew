{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from spacy.en import English as parser_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Info(object):\n",
    "    def __getitem__(self, items):\n",
    "        return (type(items), items)\n",
    "info = Info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CorpusPreprocess(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description : PreProcessing of text into following formats\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.parser_spacy = parser_spacy()\n",
    "        self.intero = \"What,Whose,Where,Why,How,Which,When,Who\".lower().split(',')\n",
    "    \n",
    "    def compounding(self,text):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        text=text.decode('utf-8')\n",
    "        \n",
    "        results=''\n",
    "        # Use Spacy Parser\n",
    "        parsed = self.parser_spacy(text.decode('utf-8'))\n",
    "\n",
    "        # Break text into sentences\n",
    "        sents = [str(x) for x in list(parsed.sents)]\n",
    "\n",
    "        for sent in sents:\n",
    "\n",
    "            \"\"\"\n",
    "            Operations on each sentence. \n",
    "            \"\"\"\n",
    "\n",
    "            sent=sent.decode('utf-8').replace('-','_')\n",
    "            temp_sent=''\n",
    "\n",
    "            for x in self.parser_spacy(sent.decode('utf-8')):\n",
    "                # Identifies the modifier of a noun and sticks them into one word\n",
    "                if x.dep_ == 'amod':\n",
    "                    temp_sent += str(x).strip()+\"|\"\n",
    "                else:\n",
    "                    temp_sent +=  str(x).strip()+\" \"\n",
    "\n",
    "            sent = temp_sent\n",
    "            tokens=[]\n",
    "            results=''\n",
    "            \n",
    "            for x in self.parser_spacy(sent.decode('utf-8')):\n",
    "\n",
    "                if x.pos_=='NOUN' or x.pos_=='PROPN':\n",
    "                    # Returns nouns as tuple of the word and it's POS tag\n",
    "                    tokens.append([str(x),'NN'])\n",
    "                else:\n",
    "                    tokens.append(\" \"+str(x)+\" \")\n",
    "\n",
    "            #print 'Tokens : ',tokens\n",
    "\n",
    "            for i,token in enumerate(tokens):\n",
    "\n",
    "                    # Combines consequtive nouns \n",
    "                    if token[1].startswith('NN'):\n",
    "                        if tokens[i-1][0].lower() not in self.intero and tokens[i][0].lower() not in self.intero:\n",
    "                            if tokens[i][1] == tokens[i-1][1]:\n",
    "                                results+=\"_\"+token[0]\n",
    "                            else:\n",
    "                                results+=\" \"+token[0]\n",
    "                        else:\n",
    "                            results+=\" \"+token[0]\n",
    "                    else:\n",
    "                        results+=\" \"+token\n",
    "\n",
    "        # Returns sentence after removing Consicutive Multiple Whitespaces\n",
    "        results = re.sub('\\s+',' ',results.strip())\n",
    "        \n",
    "        return results.decode('utf-8')\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pos_extensions(self,text):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        compounded = self.compounding(text)\n",
    "        \n",
    "        return [(str(x).decode('utf-8'),x.pos_) for x in self.parser_spacy(compounded)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cp = CorpusPreprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Bill_Gates is the richest|man on Wall_Street , who also does most|charity !'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.compounding('Bill Gates is the richest man on Wall Street, who also does most charity!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Bill_Gates', u'NOUN'),\n",
       " (u'is', u'VERB'),\n",
       " (u'the', u'DET'),\n",
       " (u'richest|man', u'NOUN'),\n",
       " (u'on', u'ADP'),\n",
       " (u'Wall_Street', u'PROPN'),\n",
       " (u',', u'PUNCT'),\n",
       " (u'who', u'NOUN'),\n",
       " (u'also', u'ADV'),\n",
       " (u'does', u'VERB'),\n",
       " (u'most|charity', u'NOUN'),\n",
       " (u'!', u'PUNCT')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.pos_extensions('Bill Gates is the richest man on Wall Street, who also does most charity!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RegexStudio(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Various Regex options for cleaning string\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        #Initialize regex(s)\n",
    "        self.regex_url = 'https*\\S*|www.\\S*.\\S'\n",
    "        self.regex_hashtag = '#[^\\s#]+'\n",
    "        self.regex_username = '@[^\\s@]+'\n",
    "        self.regex_alpha_only = '[A-Za-z]+'\n",
    "        self.regex_between_quotes = '.*\\((.*?)\\)'\n",
    "        self.regex_manage_spaces = '\\s\\s+'\n",
    "        \n",
    "    def between_substrings(self, text, s1='^', s2='$'):\n",
    "        \n",
    "        spl_chars_s1 = list(set(re.findall(\"[^A-Za-z0-9,\\s]\",s1)))\n",
    "        spl_chars_s2 = list(set(re.findall(\"[^A-Za-z0-9,\\s]\",s2)))\n",
    "        \n",
    "        for spl_chars in spl_chars_s1:\n",
    "            s1 = s1.replace(spl_chars,'\\\\'+spl_chars)\n",
    "            \n",
    "        for spl_chars in spl_chars_s2:\n",
    "            s2 = s2.replace(spl_chars,'\\\\'+spl_chars)\n",
    "        \n",
    "        regex_between_two_substrings = '.*'+s1+'(.*?)'+s2\n",
    "        \n",
    "        return re.findall(regex_between_two_substrings,text)\n",
    "    \n",
    "    def clean(self,\\\n",
    "            text,\\\n",
    "            url = False,\\\n",
    "            hashtag = False,\\\n",
    "            username = False,\\\n",
    "            alpha_only = False,\\\n",
    "            alnum = True,\\\n",
    "            between_quotes = False,\\\n",
    "            manage_spaces = True,\\\n",
    "            lower = False\n",
    "             ):\n",
    "        \n",
    "        if lower == True:\n",
    "            text = text.lower()\n",
    "        \n",
    "        if url == False:\n",
    "            text = re.sub(self.regex_url,' ',text)\n",
    "        \n",
    "        if hashtag == False:\n",
    "            text = re.sub(self.regex_hashtag,' ',text)\n",
    "            \n",
    "        if username == False:\n",
    "            text = re.sub(self.regex_username,' ',text)\n",
    "            \n",
    "        if alpha_only == False:\n",
    "            text = \" \".join(re.findall(self.regex_alpha_only,text))\n",
    "         \n",
    "        if alnum == False:\n",
    "            text = \" \".join(re.findall(self.regex_alnum,text))\n",
    "        \n",
    "        if between_quotes == False:\n",
    "            text = re.sub(self.regex_between_quotes,' ',text)\n",
    "            text.replace('( )',' ')\n",
    "            \n",
    "        if manage_spaces == True:\n",
    "            text = re.sub(self.regex_manage_spaces,' ',text)\n",
    "            \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rs = RegexStudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text = \"Bill Gates is the richest man on Wall Street 09, who also does most charity (or so he claims)!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bill Gates is the richest man on Wall Street who also does most charity or so he claims'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.clean(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StringInfo():\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts various Informations out od text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__regex_url__ = 'https*\\S*|www.\\S*.\\S'\n",
    "        self.__regex_hashtag__ = '#[^\\s#]+'\n",
    "        self.__regex_username__ = '@[^\\s@]+'\n",
    "        self.__regex_alpha_only__ = '[A-Za-z]+'\n",
    "        self.__regex_alnum__ = '[A-Za-z0-9]+'\n",
    "        self.__regex_between_quotes__ = '.*\\((.*?)\\)'\n",
    "        self.__regex_manage_spaces__ = '\\s\\s+'\n",
    "        self.full = {}\n",
    "    \n",
    "    def info(self,\\\n",
    "            text,\\\n",
    "            url = False,\\\n",
    "            hashtag = False,\\\n",
    "            username = False,\\\n",
    "            alpha_only = False,\\\n",
    "            alnum = True,\\\n",
    "            between_quotes = False,\\\n",
    "            manage_spaces = True,\\\n",
    "            lower = False\n",
    "             ):\n",
    "        self._text = text\n",
    "        if lower == True:\n",
    "            text = text.lower()\n",
    "        \n",
    "        self.url = re.findall(self.__regex_url__,text)\n",
    "        self.full['url'] = re.findall(self.__regex_url__,text)\n",
    "        \n",
    "        self.hashtag = re.findall(self.__regex_hashtag__,text)\n",
    "        self.full['hashtag'] = re.findall(self.__regex_hashtag__,text)\n",
    "            \n",
    "        self.username = re.findall(self.__regex_username__,text)\n",
    "        self.full['username'] = re.findall(self.__regex_username__,text)\n",
    "            \n",
    "        self.alpha_only = \" \".join(re.findall(self.__regex_alpha_only__,text))\n",
    "        self.full['alpha_only'] = \" \".join(re.findall(self.__regex_alpha_only__,text))\n",
    "         \n",
    "        self.alnum = \" \".join(re.findall(self.__regex_alnum__,text))\n",
    "        self.full['alnum'] = \" \".join(re.findall(self.__regex_alnum__,text))\n",
    "        \n",
    "        self.inquotes = re.findall(self.__regex_between_quotes__,text)\n",
    "        self.full['inquotes'] = re.findall(self.__regex_between_quotes__,text)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "si = StringInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "info = si.info(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['or so he claims']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.inquotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CorpusFormat(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train_test_set(self, corpus, train_ratio = 0.8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Desc : Divides the input list into training and test sets after shuffling\n",
    "        Input : Corpus to be divided\n",
    "        Output : Tuple of Training and Test set (In that order)\n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.shuffle(corpus)\n",
    "        \n",
    "        training_set_size = int(train_ratio  *len(corpus))\n",
    "        training_set = corpus[:training_set_size]\n",
    "        \n",
    "        test_set = corpus[training_set_size:]\n",
    "        \n",
    "        return (training_set,test_set)\n",
    "    \n",
    "    \n",
    "    def pad(self, string, pre_padding=1, post_padding=1 , lower=True, regex = \"[A-Za-z0-9]+\", numbers = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Desc : Divides the input list into training and test sets after shuffling\n",
    "        Input : Corpus to be divided\n",
    "        Output : Tuple of Training and Test set (In that order)\n",
    "        \"\"\"\n",
    "        \n",
    "        if lower == True:\n",
    "            string = string.lower()\n",
    "        \n",
    "        string=string.strip()\n",
    "        words = re.findall(regex,string)\n",
    "        \n",
    "        padded = words\n",
    "        \n",
    "        for x in range(pre_padding):\n",
    "                padded = ['<start>'] + padded\n",
    "        for x in range(post_padding):\n",
    "                padded = padded + ['<end>']\n",
    "        return padded\n",
    "        \n",
    "    def index(self, string, word_to_index=False, lower=True, pads=['<>'], regex = \"[A-Za-z0-9]+\", numbers = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Desc : Creates a dictionary containing indexes of the words\n",
    "        Input : Corpus (String)\n",
    "        Output : Dict[word] = respective_index\n",
    "        \"\"\"\n",
    "        \n",
    "        if lower == True:\n",
    "            string = string.lower()\n",
    "            \n",
    "        if numbers == False:\n",
    "            regex = \"[A-Za-z]\"\n",
    "        \n",
    "        for pad in pads:\n",
    "            string = re.sub(pad,'',string)\n",
    "        \n",
    "        string = string.strip()\n",
    "        \n",
    "        words = re.findall(regex,string)\n",
    "        \n",
    "        word_index = {}\n",
    "        \n",
    "        unique_words = list(set(words))\n",
    "        \n",
    "        if word_to_index == False:\n",
    "            for i,word in enumerate(unique_words):\n",
    "                word_index[word] = i\n",
    "        elif type(word_to_index) == dict:\n",
    "            for word in unique_words:\n",
    "                try:\n",
    "                    word_index[word] = word_to_index[word]\n",
    "                except KeyError:\n",
    "                    max_len = len(word_to_index)\n",
    "                    word_to_index[word] = max_len+1\n",
    "                    word_index[word] = max_len+1\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        return word_index\n",
    "    \n",
    "    \n",
    "    def corpus_info(self, corpus, lower=True, regex = \"[A-Za-z0-9]+\", numbers = True, model=False):\n",
    "    \n",
    "        \"\"\"\n",
    "        Returns Following information:\n",
    "        1. vocab_size\n",
    "        2. word_dim\n",
    "        3.  word_to_index\n",
    "        4. index_to_word\n",
    "        5. word_to_vector\n",
    "        \n",
    "        Input : string or list\n",
    "        Output : (int,int,dict,list,dict)\n",
    "        \"\"\"\n",
    "  \n",
    "        \n",
    "        try:\n",
    "            self.__dict__['model']\n",
    "        except KeyError:\n",
    "            self.model = model\n",
    "            \n",
    "            \n",
    "        flag=None\n",
    "        full_corpus_info = {}\n",
    "        print str(type(self.model))\n",
    "        if str(type(self.model)) == \"<class 'gensim.models.word2vec.Word2Vec'>\":\n",
    "            \"\"\"\n",
    "            Pre-trained gensim Word2Vec model\n",
    "            \"\"\"\n",
    "            word_dim = list(self.model[self.model.vocab.keys()[0]].shape)[0]\n",
    "            flag=0\n",
    "        \n",
    "        elif type(self.model) == dict:\n",
    "            \"\"\"\n",
    "            Dict where key is word (string) and it's value is it's respective vector (numpy array)\n",
    "            \"\"\"\n",
    "            key,value = model.iteritems()[0]\n",
    "            if type(value) == np.ndarray:\n",
    "                word_dim = len(value)\n",
    "                flag=1\n",
    "            elif type(key) == np.ndarray:\n",
    "                print \"Hint : reverse format of dict\"\n",
    "                raise TypeError\n",
    "        \n",
    "        elif type(self.model) == list:\n",
    "            \n",
    "            if type(model[0]) == tuple:\n",
    "                \"\"\"\n",
    "                List of tuples\n",
    "                Tuple of word against it's vector\n",
    "                \"\"\"\n",
    "                if (model[0][1]) == np.ndarray:\n",
    "                    word_dim = len(model[0][1])\n",
    "                    del(self.model)\n",
    "                    self.model = dict(model)\n",
    "                    flag=2\n",
    "                elif (model[0][0]) == np.ndarray:\n",
    "                    print \"Hint : reverse format of dict\"\n",
    "                    raise TypeError\n",
    "            \n",
    "            elif type(model[0]) == list and len(model[0]) == 2:\n",
    "                \"\"\"\n",
    "                List of Lists\n",
    "                First List is list of words (strings)\n",
    "                Second List is list of vectors (numpy array)\n",
    "                \"\"\"\n",
    "                if type(model[1][0]) == np.ndarray:\n",
    "                    word_dim = len(model[1][0])\n",
    "                    del(self.model)\n",
    "                    self.model = dict(zip(model[0],model[1]))\n",
    "                    flag=3\n",
    "                elif type(model[0][0]) == np.ndarray:\n",
    "                    print \"Hint : reverse format of dict\"\n",
    "                    raise TypeError\n",
    "                \n",
    "        else:\n",
    "            print \"Hint : format of model not supported\"\n",
    "            raise TypeError\n",
    "            \n",
    "        full_corpus_info['word_dim'] = word_dim\n",
    "            \n",
    "        \n",
    "        if type(corpus) == str:\n",
    "            string = corpus\n",
    "            if lower == True:\n",
    "                string = string.lower()\n",
    "            if numbers == False:\n",
    "                regex = \"[A-Za-z]\"\n",
    "            words = re.findall(regex,string)\n",
    "        \n",
    "        \n",
    "        elif type(corpus) == list:\n",
    "            string = ' '.join(corpus)\n",
    "            if lower == True:\n",
    "                string = string.lower()\n",
    "            if numbers == False:\n",
    "                regex = \"[A-Za-z]\"\n",
    "            words = re.findall(regex,string)\n",
    "        \n",
    "        index_to_word = list(set(words))\n",
    "        full_corpus_info['index_to_word'] = index_to_word\n",
    "\n",
    "        vocab_size = len(index_to_word)\n",
    "        full_corpus_info['vocab_size'] = vocab_size\n",
    "\n",
    "        word_to_index={}\n",
    "        word_to_vector = {}\n",
    "        \n",
    "        if model:\n",
    "            for i,word in enumerate(index_to_word):\n",
    "                word_to_index[word] = i\n",
    "                try:\n",
    "                    word_to_vector[word] = self.model[word]\n",
    "                except KeyError:\n",
    "                    word_to_vector[word] = np.zeros(word_dim)\n",
    "\n",
    "            full_corpus_info['word_to_index'] = word_to_index\n",
    "            full_corpus_info['index_to_word'] = index_to_word\n",
    "        \n",
    "        summary = 'vocab size : '+str(vocab_size)+\"\\n\"+\"dimensions of words : \"+str(word_dim)\n",
    "        full_corpus_info['summary'] = summary\n",
    "        \n",
    "        return full_corpus_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
